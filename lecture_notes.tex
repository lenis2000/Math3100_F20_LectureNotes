\documentclass[letterpaper,11pt,oneside,reqno]{amsart}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%bibliography
\usepackage[sorting=nyt,style=alphabetic,backend=bibtex,hyperref=true,doi=false,maxbibnames=9,maxcitenames=4]{biblatex}
\makeatletter
\def\blx@maxline{77}
\makeatother
\addbibresource{~/Dropbox/BiBTeX/bib.bib}
\sloppy

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%main packages
\usepackage{amsmath,amssymb,amsthm,amsfonts}
\usepackage[colorlinks=true,linkcolor=blue,citecolor=red]{hyperref}
\usepackage{graphicx,color}
\usepackage{upgreek}
\usepackage[mathscr]{euscript}

%equations
\allowdisplaybreaks
\numberwithin{equation}{section}

%tikz
\usepackage{tikz}
\usetikzlibrary{shapes,arrows,positioning,decorations.markings}

%conveniences
\usepackage{array}
\usepackage{adjustbox}
\usepackage{cleveref}
\usepackage{enumerate}

%paper geometry
\usepackage[DIV=13]{typearea}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%draft-specific
\newcommand{\note}[1]{ {\color{blue}\textsf{(#1)}}}
\synctex=1
% \usepackage{refcheck,comment}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%this paper specific

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newtheorem{proposition}{Proposition}[section]
\newtheorem{lemma}[proposition]{Lemma}
\newtheorem{corollary}[proposition]{Corollary}
\newtheorem{theorem}[proposition]{Theorem}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{definition}
\newtheorem{definition}[proposition]{Definition}
\newtheorem{remark}[proposition]{Remark}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}
\title{MATH 3100 Fall 2020. Lecture summaries}

% OTHER AUTHORS 

\author{Leonid Petrov}

\date{}

\maketitle

\section{8/25}

Section 1.1 in the textbook.

\begin{enumerate}
	\item Sample space --- long abstract definition which encompasses 
		all possible mathematical models of randomness we are going to see in the course
	\item Examples of sample spaces --- coin tossing, dice rolling. 
	\item We are discussing finite sample spaces so far.
		Out of finite sample spaces, a special case is formed by 
		\emph{finite sample spaces with equally likely outcomes}.
		In them, we have $\mathsf{P}(\omega)=\dfrac1{\#\Omega}$ for all
		$\omega\in \Omega$, 
		and $\mathsf{P}(A)=\dfrac{\#A}{\#\Omega}$ for all events $A$.
	\item Repeated experiments, sample space $\Omega^n=\Omega\times \ldots\times \Omega $
		(Cartesian power), where
		\begin{equation*}
			\Omega^n = \left\{ (a_1,\ldots,a_n )\colon a_i\in \Omega \right\}
		\end{equation*}
		is the space of ordered $n$-tuples of elements from $\Omega$. 
		The sample space $\Omega^n$ models the experiment corresponding to $\Omega$,
		repeated (independently) $n$ times.
	\item Finer point. In uncountable sample spaces, usually it is not possible to 
		define $\mathsf{P}$ consistently for all subsets. Therefore, 
		we need to restrict the definition of event to ``good'' subsets of $\Omega$.
\end{enumerate}

\section{8/27}

Section 1.2 in the textbook.

\begin{enumerate}
	\item Random sampling. We stay in the scenario with finite sample spaces, 
		equally likely outcomes. 
	\item 
		We discuss three main sampling schemes of $k$ objects out of $n$ objects.
	\item If we sample with replacement and order matters, then
		$\#\Omega=n^k$
	\item If we sample without replacement and order matters, then
		$$\#\Omega=n(n-1)\ldots(n-k+1)=\dfrac{n!}{(n-k)!}. $$
		If $k=n$, we talk about random permutations of $n$ objects.
	\item If we sample without replacement and order does not matter, then
		$$\#\Omega=\frac{n(n-1)\ldots(n-k+1)}{k!}=\dfrac{n!}{(n-k)!\,k!}=\binom nk .$$
	\item Hypergeometric distribution. Imagine we have $n$
		objects separated into a number of types
		$1,\ldots,L $, and there are $m_j$ objects
		of type $j$. So that $m_1+\ldots+m_L =n$.
		Sample $k$ objects at random from $n$.
		The probability that in this sample there 
		are $p_j$ objects of type $j$ is equal to 
		\begin{equation*}
			\frac{\dbinom{m_1}{p_1}\ldots\dbinom{p_L}{m_L} }{\dbinom nk},
		\end{equation*}
		where $p_1+\ldots+p_L=k $.

\end{enumerate}


\section{9/1}

Sections 1.3 and 1.4 in the textbook.

\begin{enumerate}
	\item Geometric distribution $\mathsf{P}(k)=p^{k-1}(1-p)$, $k=1,2,\ldots $.
		This is an example of an infinite $\Omega$. Here $\Omega$ is countable.
		Countable and finite sample spaces have a special unifying name,
		``discrete sample spaces''.

	\item Geometric series, its sum $=\dfrac{\textnormal{first term}}{1-\textnormal{ratio}}$.
	\item Continuous uniform distribution on $[0,1]$ --- another example of an infinite $\Omega$.
		This $\Omega$ is uncountable.
	\item $\mathsf{P}(A)$ behaves like area of the event, both in continuous uniform case
		and in general (in some sense).
	\item Operations on events and their probabilities: 
		decomposition, complement, monotonicity, inclusion-exclusion.
\end{enumerate}

\section{9/3}

Sections 1.4 and 1.5 in the textbook.

\begin{enumerate}
	\item Operations on events and their probabilities, and 
		corresponding examples: decomposition, complements, monotonicity, inclusion-exclusion.
	\item For monotonicity, a proof that we will see T with probability 1, after repeatedly 
		tossing a fair coin.
	\item For inclusion-exclusion, discussed a hard problem of computing the 
		probability that no one has their own hat, if the hats are randomly permuted.
	\item Random variable is a function on the sample space.
		This is the second fundamental definition 
		of the course.
	\item Discussed the definition, examples of random variables
		on discrete and continuous sample spaces.
	\item Probability mass function (for discrete random variables).
		Probability distribution.
\end{enumerate}

\section{9/8}

Conditional probability and Bayes' rule (Sections 2.1 and 2.2)
\begin{enumerate}
	\item Definition of conditional probability $\mathsf{P}(A\mid B)$
	\item Multiplication rule
		$\mathsf{P}(AB)=\mathsf{P}(B)\, \mathsf{P}(A\mid B)$.
	\item Law of total probability
		$\mathsf{P}(A)=\sum_{k=1}^{N}\mathsf{P}(A\mid B_k)\,\mathsf{P}(B_k)$,
		where $\Omega=\bigcup_{k=1}^{N}B_k$ is a partition of the sample space.
	\item Bayes's formula 
		$$\mathsf{P}(B \mid A)=\frac{\mathsf{P}(A\mid B)\mathsf{P}(B)}{\mathsf{P}(A)}=
		\frac{\mathsf{P}(A\mid B)\mathsf{P}(B)}
		{\mathsf{P}(A\mid B)\mathsf{P}(B)
		+
		\mathsf{P}(A\mid B^c)\mathsf{P}(B^c)}.
		$$
\end{enumerate}








\section{9/10}

Independence (section 2.3).

\begin{enumerate}
	\item Some hints on the most challenging problems from Problem Set 3.
	\item Independence. Independence algebraically means product rule.
	\item Independence of two events.
	\item Mutual independence and pairwise independence of several events.
	\item Electric circuits example.
	\item Model of 
		arbitrary many
		independent events with $\mathsf{P}(A_j)=\frac{1}{2}$ on $\Omega=[0,1]$.
	\item Independence of random variables.

\end{enumerate}

\section{9/15}

Section 2.4 and some parts from 2.5.

\begin{enumerate}
	\item Recall independence.
	\item Independent events from independent events, for example, 
		$AB^c$ and $C^c$ are independent if $A,B,C$ are mutually independent.
	\item Independent trials. Sample space.
	\item Proof that all the probabilities sum to one.
	\item Bernoulli, binomial, geometric distributions.
	\item Conditional independence (brief discussion).
	\item Hypergeometric distribution.
\end{enumerate}

\section{9/17}

Section 3.1.

\begin{enumerate}
	\item Probability mass function, pmf (for discrete distributions). Examples, properties.
	\item Probability density function, pdf (for continuous distributions).
		Examples, properties. 
	\item (Continuous) uniform distribution.
	\item Pdf as a derivative / infinitesimal description.
	\item Example with a uniform point in a disc and the pdf for $R$, the distance from the 
		point to $0$.
\end{enumerate}


\section{9/24}

Sections 3.2 and 3.3.

\begin{enumerate}
	\item Cumulative distribution function (cdf)
	\begin{itemize}
		\item Motivation for cdf
		\item Definition of cdf
		\item Relation between cdf and pmf for discrete random variables
		\item Relation between cdf and pdf for continuous random variables
		\item Properties of the cdf
	\end{itemize}
	\item Expectation
\end{enumerate}


% \section{9/29}
% \section{10/1}
% \section{10/6}
% \section{10/8}
% \section{10/13}
% \section{10/15}
% \section{10/20}
% \section{10/27}
% \section{10/29}
% \section{11/3}
% \section{11/5}
% \section{11/10}
% \section{11/12}
% \section{11/17}
% \section{11/19}
% \section{11/24}
%











































\end{document}
