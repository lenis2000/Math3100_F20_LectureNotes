\documentclass[letterpaper,11pt,oneside,reqno]{amsart}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%bibliography
\usepackage[sorting=nyt,style=alphabetic,backend=bibtex,hyperref=true,doi=false,maxbibnames=9,maxcitenames=4]{biblatex}
\makeatletter
\def\blx@maxline{77}
\makeatother
\addbibresource{~/Dropbox/BiBTeX/bib.bib}
\sloppy

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%main packages
\usepackage{amsmath,amssymb,amsthm,amsfonts}
\usepackage[colorlinks=true,linkcolor=blue,citecolor=red]{hyperref}
\usepackage{graphicx,color}
\usepackage{upgreek}
\usepackage[mathscr]{euscript}

%equations
\allowdisplaybreaks
\numberwithin{equation}{section}

%tikz
\usepackage{tikz}
\usetikzlibrary{shapes,arrows,positioning,decorations.markings}

%conveniences
\usepackage{array}
\usepackage{adjustbox}
\usepackage{cleveref}
\usepackage{enumerate}

%paper geometry
\usepackage[DIV=13]{typearea}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%draft-specific
\newcommand{\note}[1]{ {\color{blue}\textsf{(#1)}}}
\synctex=1
% \usepackage{refcheck,comment}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%this paper specific

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newtheorem{proposition}{Proposition}[section]
\newtheorem{lemma}[proposition]{Lemma}
\newtheorem{corollary}[proposition]{Corollary}
\newtheorem{theorem}[proposition]{Theorem}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{definition}
\newtheorem{definition}[proposition]{Definition}
\newtheorem{remark}[proposition]{Remark}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}
\title{MATH 3100 Fall 2020. Lecture summaries}

% OTHER AUTHORS 

\author{Leonid Petrov}

\date{}

\maketitle

\section{8/25}

Section 1.1 in the textbook.

\begin{enumerate}
	\item Sample space --- long abstract definition which encompasses 
		all possible mathematical models of randomness we are going to see in the course
	\item Examples of sample spaces --- coin tossing, dice rolling. 
	\item We are discussing finite sample spaces so far.
		Out of finite sample spaces, a special case is formed by 
		\emph{finite sample spaces with equally likely outcomes}.
		In them, we have $\mathsf{P}(\omega)=\dfrac1{\#\Omega}$ for all
		$\omega\in \Omega$, 
		and $\mathsf{P}(A)=\dfrac{\#A}{\#\Omega}$ for all events $A$.
	\item Repeated experiments, sample space $\Omega^n=\Omega\times \ldots\times \Omega $
		(Cartesian power), where
		\begin{equation*}
			\Omega^n = \left\{ (a_1,\ldots,a_n )\colon a_i\in \Omega \right\}
		\end{equation*}
		is the space of ordered $n$-tuples of elements from $\Omega$. 
		The sample space $\Omega^n$ models the experiment corresponding to $\Omega$,
		repeated (independently) $n$ times.
	\item Finer point. In uncountable sample spaces, usually it is not possible to 
		define $\mathsf{P}$ consistently for all subsets. Therefore, 
		we need to restrict the definition of event to ``good'' subsets of $\Omega$.
\end{enumerate}

\section{8/27}

Section 1.2 in the textbook.

\begin{enumerate}
	\item Random sampling. We stay in the scenario with finite sample spaces, 
		equally likely outcomes. 
	\item 
		We discuss three main sampling schemes of $k$ objects out of $n$ objects.
	\item If we sample with replacement and order matters, then
		$\#\Omega=n^k$
	\item If we sample without replacement and order matters, then
		$$\#\Omega=n(n-1)\ldots(n-k+1)=\dfrac{n!}{(n-k)!}. $$
		If $k=n$, we talk about random permutations of $n$ objects.
	\item If we sample without replacement and order does not matter, then
		$$\#\Omega=\frac{n(n-1)\ldots(n-k+1)}{k!}=\dfrac{n!}{(n-k)!\,k!}=\binom nk .$$
	\item Hypergeometric distribution. Imagine we have $n$
		objects separated into a number of types
		$1,\ldots,L $, and there are $m_j$ objects
		of type $j$. So that $m_1+\ldots+m_L =n$.
		Sample $k$ objects at random from $n$.
		The probability that in this sample there 
		are $p_j$ objects of type $j$ is equal to 
		\begin{equation*}
			\frac{\dbinom{m_1}{p_1}\ldots\dbinom{p_L}{m_L} }{\dbinom nk},
		\end{equation*}
		where $p_1+\ldots+p_L=k $.

\end{enumerate}


\section{9/1}

Sections 1.3 and 1.4 in the textbook.

\begin{enumerate}
	\item Geometric distribution $\mathsf{P}(k)=p^{k-1}(1-p)$, $k=1,2,\ldots $.
		This is an example of an infinite $\Omega$. Here $\Omega$ is countable.
		Countable and finite sample spaces have a special unifying name,
		``discrete sample spaces''.

	\item Geometric series, its sum $=\dfrac{\textnormal{first term}}{1-\textnormal{ratio}}$.
	\item Continuous uniform distribution on $[0,1]$ --- another example of an infinite $\Omega$.
		This $\Omega$ is uncountable.
	\item $\mathsf{P}(A)$ behaves like area of the event, both in continuous uniform case
		and in general (in some sense).
	\item Operations on events and their probabilities: 
		decomposition, complement, monotonicity, inclusion-exclusion.
\end{enumerate}

\section{9/3}

Sections 1.4 and 1.5 in the textbook.

\begin{enumerate}
	\item Operations on events and their probabilities, and 
		corresponding examples: decomposition, complements, monotonicity, inclusion-exclusion.
	\item For monotonicity, a proof that we will see T with probability 1, after repeatedly 
		tossing a fair coin.
	\item For inclusion-exclusion, discussed a hard problem of computing the 
		probability that no one has their own hat, if the hats are randomly permuted.
	\item Random variable is a function on the sample space.
		This is the second fundamental definition 
		of the course.
	\item Discussed the definition, examples of random variables
		on discrete and continuous sample spaces.
	\item Probability mass function (for discrete random variables).
		Probability distribution.
\end{enumerate}

\section{9/8}

Conditional probability and Bayes' rule (Sections 2.1 and 2.2)
\begin{enumerate}
	\item Definition of conditional probability $\mathsf{P}(A\mid B)$
	\item Multiplication rule
		$\mathsf{P}(AB)=\mathsf{P}(B)\, \mathsf{P}(A\mid B)$.
	\item Law of total probability
		$\mathsf{P}(A)=\sum_{k=1}^{N}\mathsf{P}(A\mid B_k)\,\mathsf{P}(B_k)$,
		where $\Omega=\bigcup_{k=1}^{N}B_k$ is a partition of the sample space.
	\item Bayes's formula 
		$$\mathsf{P}(B \mid A)=\frac{\mathsf{P}(A\mid B)\mathsf{P}(B)}{\mathsf{P}(A)}=
		\frac{\mathsf{P}(A\mid B)\mathsf{P}(B)}
		{\mathsf{P}(A\mid B)\mathsf{P}(B)
		+
		\mathsf{P}(A\mid B^c)\mathsf{P}(B^c)}.
		$$
\end{enumerate}








\section{9/10}

Independence (section 2.3).

\begin{enumerate}
	\item Some hints on the most challenging problems from Problem Set 3.
	\item Independence. Independence algebraically means product rule.
	\item Independence of two events.
	\item Mutual independence and pairwise independence of several events.
	\item Electric circuits example.
	\item Model of 
		arbitrary many
		independent events with $\mathsf{P}(A_j)=\frac{1}{2}$ on $\Omega=[0,1]$.
	\item Independence of random variables.

\end{enumerate}

\section{9/15}

Section 2.4 and some parts from 2.5.

\begin{enumerate}
	\item Recall independence.
	\item Independent events from independent events, for example, 
		$AB^c$ and $C^c$ are independent if $A,B,C$ are mutually independent.
	\item Independent trials. Sample space.
	\item Proof that all the probabilities sum to one.
	\item Bernoulli, binomial, geometric distributions.
	\item Conditional independence (brief discussion).
	\item Hypergeometric distribution.
\end{enumerate}

\section{9/17}

Section 3.1.

\begin{enumerate}
	\item Probability mass function, pmf (for discrete distributions). Examples, properties.
	\item Probability density function, pdf (for continuous distributions).
		Examples, properties. 
	\item (Continuous) uniform distribution.
	\item Pdf as a derivative / infinitesimal description.
	\item Example with a uniform point in a disc and the pdf for $R$, the distance from the 
		point to $0$.
\end{enumerate}


\section{9/24}

Sections 3.2 and 3.3.

\begin{enumerate}
	\item Cumulative distribution function (cdf)
	\begin{itemize}
		\item Motivation for cdf
		\item Definition of cdf
		\item Relation between cdf and pmf for discrete random variables
		\item Relation between cdf and pdf for continuous random variables
		\item Properties of the cdf
	\end{itemize}
\item Expectation.
	\begin{itemize}
		\item Expectation of a discrete random variable
		\item Expectation of a continuous random variable
		\item Expectation of geometric, Bernoulli, binomial random variables.
			Method of derivatives.
		\item Formula for the expectation of a function of a random
			variable.
		\item Nonexistence of expectation.
	\end{itemize}
\end{enumerate}

\section{9/29}

Sections 3.3 and 3.4 (further discussion of expectation, and 
variance).
\begin{enumerate}
	\item Properties of random variables (table from the textbook)
	\item Expectation which is infinite. 
		Expectation of the hypergeometric distribution.
	\item Indicator random variables.
	\item Expectation of a function of a random variable (further discussion).
	\item (begin part 2 of the video)
		Variance - definition and formula $E(X^2)-(EX)^2$.
	\item Variance of Bernoulli and binomial random variables.
	\item Hypergeometric variance (no computation, just showing you the formula which is quite complex).
	\item Expectation and variance of $aX+b$.
	\item $Var(X)=0$ if and only if $P(X=a)=1$ for some $a$.
	\item Variance of geometric and uniform distributions.
\end{enumerate}

\section{10/1}

Gaussian (normal) distribution. Sections 3.5.
\begin{enumerate}
	\item Gaussian distribution --- standard $\mathcal{N}(0,1)$
	\item Getting the probability density normalizing constant $\sqrt{2\pi}$
	\item Examples with the table
	\item Expectation and variance of the standard normal random variable
	\item Generic normal random variable $\mathcal{N}(\mu,\sigma^2)$
\end{enumerate}

\section{10/6}

Central limit theorem and law of large numbers (all for the binomial distribution).

\begin{enumerate}
	\item Graphs of binomial pmf for large $n$
	\item CLT: formulation. Limit, and normal approximation with $\Phi(x)$.
	\item CLT: examples
	\item Continuity correction
	\item CLT: idea of proof
	\item Law of large numbers for the binomial distribution
\end{enumerate}

\section{10/8}

Applications of the Central Limit Theorem (section 4.3).
\begin{enumerate}
	\item Confidence intervals for the unknown $p$ of the binomial distribution,
		\begin{equation*}
			P(|p-\hat p|>\varepsilon) \ge 2\Phi(2\varepsilon\sqrt n)-1
		\end{equation*}
	\item Various examples with confidence intervals
	\item Maximum likelihood estimate
	\item One more example of the use of Central Limit Theorem (airplane overbooking problem)
\end{enumerate}

\section{10/13}

Poisson distribution, Poisson process, exponential distribution.
Sections 4.4 -- 4.6.

\begin{enumerate}
	\item Poisson distribution
	\item Poisson distribution: mean and variance
	\item Poisson approximation to the binomial distribution
	\item Poisson approximation vs normal approximation
	\item Poisson process --- idea from bus arrivals
	\item Exponential distribution, derivation from the bus arrival process
	\item Exponential distribution: mean and variance
\end{enumerate}

\section{10/15}

Exponential distribution, Poisson process, gamma distribution
(sections 4.4--4.6).

\begin{enumerate}
	\item Exponential distribution. Definition and examples
	\item Memorylessness of the exponential distribution
	\item Exponential as a limit of geometric distribution (connection to the coin-flipping setup)
	\item Poisson process on the line, rigorous definition
	\item Poisson process, example
	\item Poisson process in space (illustration)
	\item Gamma distribution from Poisson process
	\item Binomial distribution from Poisson process
\end{enumerate}

\section{10/20}

Finishing up chapter 4, and surveying the material of chapter 5 (which we mostly skip).
Preparing for chapter 6.

\begin{enumerate}
	\item Expectation and variance of the Gamma distribution
	\item Section 5.1 --- moment generating function and its usefulness in computing all 
		moments $\mathbb{E}(X^n)$ of the 
		random variable.
	\item Section 5.2 --- Distribution of a function of a random variable. How the pdf is transformed.
	\item Overview of the topics of chapter 6.
		Beginning of discrete joint distributions.
\end{enumerate}

\section{10/27}

Joint distributions. Sections 6.1--6.2.
\begin{enumerate}
	\item Joint pmf (discrete random variables)
	\item Multinomial distribution
	\item Examples on the multinomial distribution
	\item Joint pdf (continuous random variables)
	\item Nonexistence of some joint pdfs (example with $X=Y$)
	\item Example. Crash course on double integrals $\iint$
	\item Marginal pdfs and one more example
\end{enumerate}

\section{10/29}

Joint distributions. Sections 6.2--6.3.
\begin{enumerate}
	\item Joint uniform distribution in a region $D$ of $\mathbb{R}^2$ or higher dimensions.
	\item 
		Examples. Marginal distributions.
	\item Joint pdf/pmf and independence.
	\item Functions of independent collections. Minimum and maximum.
	\item Minimum and maximum of uniform and exponential random variables.
	\item Product rule for expectations.
\end{enumerate}


\section{11/3}

Convolution of random variables (distribution of $X+Y$, where $X,Y$ are independent).
Many examples. (Section 7.1).
Poisson process via convolution (section 7.3).

\section{11/5}

Expectation, variance, and covariance for multiple random variables.
Sections 8.1, 8.2

\begin{enumerate}
	\item Linearity of expectation, for any random variables
		(section 8.1). Application to binomial and hypergeometric 
		distributions'
		expectations.
	\item Product rule for expectations for independent random
		variables (section 8.2). 
		Summation rule for variances. 
		Application to binomial distribution's variance.
		Sample mean.
	\item Covariance, some properties (continued next time).
\end{enumerate}


\section{11/10}

Covariance and correlation coefficient. Bivariate normal random variables.
Section 8.4, and Pitman's textbook (which is optional but has a somewhat shorter exposition).
\begin{enumerate}
	\item Covariance and its properties
	\item Covariance for multinomial random variables
	\item Correlation coefficient
	\item Bivariate normal: independent standard
	\item Bivariate normal: dependent standard
	\item Bivariate normal: general
	\item Example problem (heights of parents and children)
\end{enumerate}

\section{11/12}

Inequalities, law of large numbers. Sections 9.1, 9.2
\begin{enumerate}
	\item Markov inequality, proof
	\item Chebyshev inequality, proof
	\item Law of large numbers, proof
\end{enumerate}

\section{11/17}

Central limit theorem and examples (Section 9.3)
\begin{enumerate}
	\item Central limit theorem (CLT) --- binomial and general
	\item Many examples of uses of CLT
	\item Normal approximations of some known distributions (Poisson, gamma)
	\item New topic, conditioning. First example.
\end{enumerate}

\section{11/19}

Conditioning (10.1 -- 10.2).
\begin{enumerate}
	\item 
\end{enumerate}

\section{11/24}














































\end{document}
